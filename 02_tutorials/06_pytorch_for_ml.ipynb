{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "[![GPU compute on Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lento234/ml-tutorials/blob/main/01-basics/01-linear_regression.ipynb)\n",
    "\n",
    "**References**\n",
    "- https://pytorch.org/\n",
    "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "- https://en.wikipedia.org/wiki/Linear_regression\n",
    "- https://en.wikipedia.org/wiki/Backpropagation\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression**\n",
    "\n",
    "Given a data set $\\{y_i, x_i\\}_{i=1}^n$ of $n$ statistical units, we have the following relationship:\n",
    "\n",
    "$$ y_i = \\theta_1 x_i + \\theta_0 + \\varepsilon_i, \\qquad i=1,\\ldots, n,$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $y_i$ is observed value (dependent variable\n",
    "- $x_i$ is the independent variable\n",
    "- $\\theta_1$ and $\\theta_0$ are model parameters\n",
    "- $\\varepsilon_i$ is the measurement error\n",
    "\n",
    "**Table of Content**\n",
    "1. [Setup environment](#setup)\n",
    "2. [Define the model, loss function and optimizer](#model)\n",
    "3. [Train the model](#train)\n",
    "4. [Predict and evaluate the model](#evaluate)\n",
    "5. [Save the trained model](#save)\n",
    "6. [Load pretrained model](#load)\n",
    "\n",
    "[^1]: https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-poster')\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "mpl.rcParams['figure.figsize'] = 5 * np.array([1.618033988749895, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 234\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "theta_1 = 2.0\n",
    "theta_0 = 0.4\n",
    "eps_std = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(n,1)-0.5\n",
    "eps = np.random.normal(scale=eps_std, size=(n,1))\n",
    "y_train = x_train*theta_1 + theta_0 + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'k.', label='observations')\n",
    "ax.plot(x_train, x_train*theta_1 + theta_0, 'tab:gray', lw=5,\n",
    "        label=r'analytical ($\\theta_1={:.2f}$, $\\theta_0={:.2f}$)'.format(theta_1, theta_0))\n",
    "ax.set(xlabel='$x$', ylabel='$y$', xlim=(-0.52,0.52), ylim=(-1.0, 1.5));\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.7, 0.6));\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 2. Define the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1 # number of nodes (1 weight + 1 bias)\n",
    "num_epochs = 150 # i.e. number of iterations\n",
    "learning_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "$$ \\mathcal{F}(x;\\ \\theta): x \\rightarrow y $$\n",
    "\n",
    "**Linear 1-D model**:\n",
    "\n",
    "$$ \\mathcal{F}(x;\\ \\theta) = w x + b = \\hat{y}$$\n",
    "\n",
    "where:\n",
    "- $y$ is the ground truth\n",
    "- $\\hat{y}$ is the predicted output\n",
    "- $\\theta$ are the trainable parameters, with\n",
    "    - $w$ is the learnable **weight**, i.e. $w \\equiv \\theta_1$\n",
    "    - $b$ is the learnable **bias**, i.e. $b \\equiv \\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "model = nn.Linear(in_features=n_features, out_features=n_features)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(r\"{:6s} (theta_{}): {:.4f}\".format(name, 1-i, param.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "**$L^1$-norm:**\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{Y}, Y;\\ \\theta) = \\textrm{mean} \\left( \\{l_1,\\dots,l_N\\}^\\top \\right), \\quad l_n = \\left| \\hat{y}_n - y_n \\right| $$\n",
    "where:\n",
    "- $X \\in \\mathbb{R}^N$ is the input matrix\n",
    "- $Y \\in \\mathbb{R}^N$ is the ground truth matrix\n",
    "- $N$ is the batch size (for now, batch size = number of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss() # L^1-norm, aka. mean absolute error loss\n",
    "criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**Stochastic gradient descent**\n",
    "\n",
    "$$ \\theta^{n+1} = \\theta^n - \\eta \\nabla \\mathcal{L}(\\theta^n) $$\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the learning rate (i.e. step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic gradient-descent\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 3. Train the model\n",
    "\n",
    "$$ \\mathcal{F}^* = \\arg \\min_{\\mathcal{F}}\\ \\mathcal{L}(\\hat{Y}, Y;\\ \\theta)$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Convert data to torch tensor: `torch.from_numpy(...)`\n",
    "2. Forward pass: `y_hat = model(x)`\n",
    "3. Calculate loss: `loss = criterion(y_hat, y)`\n",
    "4. Compute gradients: `loss.backward()`\n",
    "5. Update weights: `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dict(loss=[], parameters=[], grads=[])\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Convert numpy arrays to torch tensors\n",
    "    x = torch.from_numpy(x_train.astype('float32'))\n",
    "    y = torch.from_numpy(y_train.astype('float32'))\n",
    "\n",
    "    # 2. Forward-pass:\n",
    "    y_hat = model(x) # prediction step\n",
    "    \n",
    "    # 3. Calculate loss\n",
    "    loss = criterion(y_hat, y)\n",
    "    \n",
    "    # 4. Backward propagation \n",
    "    optimizer.zero_grad() # reset gradients to zero\n",
    "    loss.backward() # backprop: calculate gradients w.r.t to loss\n",
    "    \n",
    "    # 5. Update weights\n",
    "    optimizer.step() # update gradient\n",
    "    \n",
    "    # 6. Log\n",
    "    history['loss'].append(loss.item())\n",
    "    history['parameters'].append([param.detach().item() for param in model.parameters()])\n",
    "    history['grads'].append([param.grad.detach().item() for param in model.parameters()])\n",
    "    if (epoch % 10) == 0 or epoch==(num_epochs-1):\n",
    "        print('[Epoch {:3d}]: loss={:.4f}, w={:.2f}, b={:.2f}'.format(\n",
    "            epoch, history['loss'][-1], *history['parameters'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(mpl.rcParams['figure.figsize'][0]*2, mpl.rcParams['figure.figsize'][1]))\n",
    "\n",
    "axes[0].plot(history['loss'])\n",
    "axes[0].set(xlabel='epoch', ylabel='$L^1$ loss',\n",
    "       title='Training loss history');\n",
    "\n",
    "axes[1].plot(history['grads'])\n",
    "axes[1].set(xlabel='epoch', ylabel='gradients',\n",
    "            title='Training gradient history');\n",
    "axes[1].legend([r'$\\nabla w \\equiv \\nabla \\theta_1$',\n",
    "                r'$\\nabla b \\equiv \\nabla \\theta_0$'])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization over the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L^1 norm: numpy version\n",
    "l1_norm = lambda y_hat, y: np.abs(y_hat - y).mean(axis=0)\n",
    "\n",
    "# Loss landscape\n",
    "theta_1_h, theta_0_h = np.meshgrid(np.linspace(0, 4),\n",
    "                                 np.linspace(-0.2, 0.8))\n",
    "loss_surface = l1_norm(x_train[...,None]*theta_1_h[None] + theta_0_h[None],\n",
    "                       y_train[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.contourf(theta_1_h, theta_0_h, loss_surface)\n",
    "ax.plot(*history['parameters'][0], 'c.', ms=15, label='start', zorder=10)\n",
    "ax.plot(theta_1, theta_0, 'r.', ms=20, label='optima', zorder=10)\n",
    "ax.plot(np.array(history['parameters'])[:,0],\n",
    "        np.array(history['parameters'])[:,1],\n",
    "        '.-', c='k', label='path')\n",
    "ax.set(xlabel=r'$\\theta_1$ ($w$, weight)',\n",
    "       ylabel=r'$\\theta_0$ ($b$, bias)',\n",
    "       title='$L^1$ loss landscape')\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.3, 0.7));\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## 4. Predict and evaluate the model\n",
    "\n",
    "### Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predict = model(torch.from_numpy(x_train.astype('float32'))).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'k.', label='observations')\n",
    "ax.plot(x_train, x_train*theta_1 + theta_0, 'tab:gray', lw=5,\n",
    "        label=r'analytical: ($\\theta_1={:.2f}$, $\\theta_0={:.2f}$)'.format(theta_1, theta_0))\n",
    "ax.plot(x_train, x_train*history['parameters'][0][0] + history['parameters'][0][1], 'tab:blue',\n",
    "        label=r'prediction [epoch 0]: ($\\theta_1={:.2f}$, $\\theta_0={:.2f}$)'.format(*history['parameters'][0]))\n",
    "ax.plot(x_train, predict, 'tab:red',\n",
    "        label=r'prediction [trained]: ($\\theta_1={:.2f}$, $\\theta_0={:.2f}$)'.format(*history['parameters'][-1]))\n",
    "ax.set(xlabel='$x$', ylabel='$y$',\n",
    "       title='Training accuracy',\n",
    "       xlim=(-0.52,0.52), ylim=(-1.0, 1.5))\n",
    "\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.9, 0.6));\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 5. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head model.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 6. Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model.ckpt')\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
